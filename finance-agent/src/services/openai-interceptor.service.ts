/**
 * Service d'interception des appels OpenAI
 * Intercepte les appels, estime les tokens et g√©n√®re des recommandations d'optimisation
 */

import { openaiTracker } from './openai-tracker.service';

/**
 * Service d'interception des appels OpenAI
 * Estime les tokens, track les appels et g√©n√®re des recommandations
 */
export class OpenAIInterceptorService {
  /**
   * Estime le nombre de tokens dans un texte
   * Utilise une approximation : ~3.5 caract√®res = 1 token (moyenne pour texte mixte)
   * Note: Cette estimation est approximative, les tokens r√©els peuvent varier
   * 
   * @param text Texte √† analyser
   * @returns Nombre estim√© de tokens
   */
  estimateTokens(text: string): number {
    if (!text || text.length === 0) {
      return 0;
    }

    // Approximation: ~3.5 caract√®res par token pour texte mixte
    // Pour √™tre plus pr√©cis, on compte les mots et caract√®res
    const words = text.trim().split(/\s+/).length;
    const chars = text.length;
    
    // Estimation bas√©e sur une moyenne entre estimation par mots et par caract√®res
    // Mots: ~0.75 tokens par mot en moyenne
    // Caract√®res: ~0.285 tokens par caract√®re (1/3.5)
    const tokensByWords = words * 0.75;
    const tokensByChars = chars / 3.5;
    
    // Moyenne pond√©r√©e (les mots sont g√©n√©ralement plus pr√©cis)
    return Math.ceil((tokensByWords * 0.6 + tokensByChars * 0.4));
  }

  /**
   * Track une g√©n√©ration OpenAI
   * Si les tokens r√©els ne sont pas fournis, les estime automatiquement
   * 
   * @param modelName Nom du mod√®le utilis√©
   * @param prompt Texte du prompt
   * @param response Texte de la r√©ponse
   * @param actualTokens Tokens r√©els si disponibles depuis l'API (optionnel)
   */
  trackGeneration(
    modelName: string,
    prompt: string,
    response: string,
    actualTokens?: { prompt: number; completion: number }
  ): void {
    let promptTokens: number;
    let completionTokens: number;

    if (actualTokens) {
      // Utiliser les tokens r√©els si disponibles
      promptTokens = actualTokens.prompt;
      completionTokens = actualTokens.completion;
    } else {
      // Estimer les tokens si non fournis
      promptTokens = this.estimateTokens(prompt);
      completionTokens = this.estimateTokens(response);
    }

    // Enregistrer l'appel dans le tracker
    openaiTracker.trackCall(modelName, promptTokens, completionTokens, false);
  }

  /**
   * Track une erreur d'appel API
   * 
   * @param modelName Nom du mod√®le concern√©
   * @param error Erreur g√©n√©r√©e
   */
  trackError(modelName: string, error: Error): void {
    // Enregistrer une erreur avec 0 tokens (l'appel a √©chou√©)
    openaiTracker.trackCall(modelName, 0, 0, true);
  }

  /**
   * G√©n√®re des recommandations d'optimisation bas√©es sur les statistiques actuelles
   * Analyse les patterns d'utilisation et sugg√®re des am√©liorations
   * 
   * @returns Liste de recommandations format√©es
   */
  generateOptimizationRecommendations(): string[] {
    const stats = openaiTracker.getAllStats();
    const recommendations: string[] = [];

    for (const [modelName, modelStats] of Object.entries(stats)) {
      if (modelStats.totalCalls === 0) {
        continue;
      }

      const avgTokens = modelStats.totalTokens / modelStats.totalCalls;
      const avgCost = modelStats.estimatedCost / modelStats.totalCalls;

      // Recommandation: R√©duction du prompt syst√®me si moyenne √©lev√©e
      if (avgTokens > 2000) {
        recommendations.push(
          `üîß Le mod√®le ${modelName} utilise ${Math.round(avgTokens)} tokens par appel en moyenne. ` +
          `Consid√©rez r√©duire la longueur du prompt syst√®me.`
        );
      }

      // Recommandation: Mod√®le moins cher si co√ªt √©lev√©
      if (avgCost > 0.01 && modelName !== 'gpt-4o-mini') {
        recommendations.push(
          `üí∞ Co√ªt moyen de $${avgCost.toFixed(4)} par appel pour ${modelName}. ` +
          `Envisagez un mod√®le moins cher comme gpt-4o-mini ($${(0.15 / 1_000_000 * avgTokens).toFixed(4)} estim√©).`
        );
      }

      // Recommandation: V√©rifier les erreurs
      if (modelStats.errorCount > 5) {
        recommendations.push(
          `‚ö†Ô∏è ${modelStats.errorCount} erreurs d√©tect√©es avec ${modelName}. ` +
          `V√©rifiez votre cl√© API et les limites de rate limit.`
        );
      }

      // Recommandation: Optimisation des prompts si moyenne tr√®s √©lev√©e
      if (avgTokens > 3000) {
        recommendations.push(
          `üìù Les prompts pour ${modelName} semblent tr√®s longs (${Math.round(avgTokens)} tokens moy.). ` +
          `R√©duisez le contexte ou limitez maxMemories dans character.ts.`
        );
      }
    }

    // Recommandation globale: Cache si beaucoup d'appels similaires
    const totals = this.calculateTotals(stats);
    if (totals.totalCalls > 50) {
      recommendations.push(
        `üíæ ${totals.totalCalls} appels d√©tect√©s. Envisagez d'impl√©menter un cache pour les r√©ponses similaires.`
      );
    }

    return recommendations;
  }

  /**
   * Calcule les totaux √† partir des statistiques
   * @param stats Statistiques par mod√®le
   * @returns Totaux agr√©g√©s
   */
  private calculateTotals(stats: Record<string, any>): {
    totalCalls: number;
    totalTokens: number;
    totalCost: number;
  } {
    let totalCalls = 0;
    let totalTokens = 0;
    let totalCost = 0;

    for (const modelStats of Object.values(stats)) {
      totalCalls += modelStats.totalCalls || 0;
      totalTokens += modelStats.totalTokens || 0;
      totalCost += modelStats.estimatedCost || 0;
    }

    return { totalCalls, totalTokens, totalCost };
  }
}

/**
 * Instance singleton du service d'interception
 */
export const openaiInterceptor = new OpenAIInterceptorService();

